# -*- coding: utf-8 -*-
"""NLP_Multimodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ooUu1yptUmows3qAKXS_0sRn9rxIIMjQ

**Basic pipeline**
1. Extract Information
From the .docx prescription, the model reads all the clean text.

From the .json lab file, it pulls key lab values like:

HbA1c

Fasting Blood Sugar

Postprandial Blood Sugar

Creatinine

2. Generate Features
The text is passed through a pre-trained BERT model to get a high-quality text embedding (a vector representation of the medical note).

The lab values are turned into a numeric tensor.

3. Classify Risk
These two sets of features (text and lab) are combined inside a neural network.

The model uses a few layers to learn patterns and finally outputs a prediction:

0 for Low Risk

1 for High Risk

4. Train the Model
A small simulated dataset is used here for demo purposes.

The model is trained using a basic training loop (5 epochs), and accuracy is printed after each epoch.

5. Make Predictions
After training, the model can:

Take a new .docx + .json pair.

Predict whether that patient is at Low or High Risk.

Also show the confidence (probabilities) for each class.
"""

!pip install python-docx
!pip install transformers
!pip install torch torchvision

# ===========================================
#  MULTI-MODAL MEDICAL RISK CLASSIFIER
# ===========================================

import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from docx import Document
import os
import random

# ===========================================
#  DATA LOADING UTILITIES
# ===========================================

def load_prescription_text(docx_path):
    """Extract clean text from prescription DOCX"""
    doc = Document(docx_path)
    return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])


def load_lab_data(json_path):
    """Load and return lab result dictionary from JSON"""
    with open(json_path, 'r') as f:
        return json.load(f)["lab_results"]


def extract_lab_features(lab_data):
    """Convert specific lab values into a fixed-size tensor"""
    keys = ["HbA1c", "Fasting Blood Sugar", "Postprandial Blood Sugar", "Creatinine"]
    values = [lab_data.get(k, {}).get("value", 0) for k in keys]
    return torch.tensor(values, dtype=torch.float32)


# ===========================================
#  TOKENIZER & BERT
# ===========================================

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")

def get_text_embedding(text):
    """Returns CLS token embedding from BERT"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state[:, 0, :]  # CLS token


# ===========================================
#  MULTI-MODAL MODEL
# ===========================================

class MultiModalRiskClassifier(nn.Module):
    def __init__(self, text_dim=768, lab_dim=4, hidden_dim=128):
        super().__init__()
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.lab_proj = nn.Linear(lab_dim, hidden_dim)
        self.classifier = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)  # Binary output
        )

    def forward(self, text_emb, lab_features):
        text_out = self.text_proj(text_emb)
        lab_out = self.lab_proj(lab_features)
        combined = torch.cat([text_out, lab_out], dim=1)
        return self.classifier(combined)


# ===========================================
#  DUMMY DATASET FOR TRAINING DEMO
# ===========================================

class RiskDataset(Dataset):
    def __init__(self, file_list):
        self.file_list = file_list

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        docx_path, json_path, label = self.file_list[idx]
        text = load_prescription_text(docx_path)
        labs = load_lab_data(json_path)
        lab_feats = extract_lab_features(labs)
        text_emb = get_text_embedding(text).squeeze(0)  # [768]
        return text_emb, lab_feats, torch.tensor(label, dtype=torch.long)


# ===========================================
#  TRAINING LOOP
# ===========================================

def train_model(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for text_emb, lab_feats, labels in dataloader:
        text_emb, lab_feats, labels = text_emb.to(device), lab_feats.to(device), labels.to(device)

        optimizer.zero_grad()
        logits = model(text_emb, lab_feats)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(dataloader)


def evaluate_model(model, dataloader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for text_emb, lab_feats, labels in dataloader:
            text_emb, lab_feats = text_emb.to(device), lab_feats.to(device)
            logits = model(text_emb, lab_feats)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels.to(device)).sum().item()
            total += labels.size(0)
    return correct / total


# ===========================================
#  SIMULATED TRAINING
# ===========================================

# Simulate a dataset (replace this with real data paths)
simulated_dataset = [
    ("med.docx", "lab_record.json", random.randint(0, 1)),  # random label
    ("med.docx", "lab_record.json", random.randint(0, 1)),
    ("med.docx", "lab_record.json", random.randint(0, 1))
]

# Dataset and DataLoader
dataset = RiskDataset(simulated_dataset)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MultiModalRiskClassifier().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# Train
print(" Starting Training...")
for epoch in range(5):
    loss = train_model(model, dataloader, optimizer, criterion, device)
    acc = evaluate_model(model, dataloader, device)
    print(f"Epoch {epoch+1} | Loss: {loss:.4f} | Accuracy: {acc:.2f}")

# Inference on new data
model.eval()
text = load_prescription_text("med.docx")
labs = load_lab_data("lab_record.json")
text_emb = get_text_embedding(text)
lab_feats = extract_lab_features(labs).unsqueeze(0)
with torch.no_grad():
    logits = model(text_emb, lab_feats)
    probs = torch.softmax(logits, dim=1)
    pred = torch.argmax(probs, dim=1).item()
    print("\n Final Prediction:", ["Low Risk", "High Risk"][pred])
    print(" Probabilities:", probs.tolist())